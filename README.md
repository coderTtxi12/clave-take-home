# Natural Language Dashboard Generator

A production-ready AI-powered dashboard that transforms natural language queries into dynamic visualizations from restaurant analytics data. Built for the Clave Engineering Take-Home Assessment.

## ğŸ¯ Overview

This system allows restaurant owners to ask questions in plain English like:
- *"Show me sales comparison between Downtown and Airport locations"*
- *"What were my top 5 selling products last week?"*
- *"Graph hourly sales for Friday vs Saturday at all stores"*

**The system automatically:**
1. Parses the natural language query
2. Generates Python code to query the database
3. Executes the code in a secure sandbox
4. Creates appropriate visualizations (charts, tables, metrics)
5. Returns interactive widgets to the dashboard

---

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend       â”‚  Next.js Dashboard (TypeScript)
â”‚   (Port 3000)    â”‚  - Natural language input
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Dynamic visualization rendering
         â”‚
         â”‚ HTTP/REST
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend API   â”‚  FastAPI (Python)
â”‚   (Port 8000)   â”‚  - Query processing
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Session management
         â”‚
         â”‚ OpenAI API
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Coding Agent   â”‚  Agentic Loop
â”‚                 â”‚  - Code generation
â”‚                 â”‚  - Tool execution
â”‚                 â”‚  - Context compression
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Code Execution
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Code Executor   â”‚  Isolated Docker Container
â”‚ (Port 8001)     â”‚  - Secure code execution
â”‚                 â”‚  - Database access
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ SQL Queries
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Supabase      â”‚  PostgreSQL Database
â”‚   (Production)  â”‚  - Normalized restaurant data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **User Query** â†’ Frontend sends natural language query
2. **LLM Processing** â†’ Backend uses OpenAI to interpret query
3. **Code Generation** â†’ Agent generates Python code with database queries
4. **Code Execution** â†’ Isolated executor runs code safely
5. **Data Retrieval** â†’ Executor queries Supabase database
6. **Visualization** â†’ Results formatted as charts/tables
7. **Response** â†’ Frontend renders interactive widgets

---

## ğŸ¤– The Coding Agent: Core Innovation

### Why a Coding Agent?

Traditional approaches map queries directly to SQL, but this is brittle:
- **Limited flexibility**: Can't handle complex data transformations
- **No iteration**: Can't refine queries based on results
- **Fixed patterns**: Struggles with ambiguous or novel requests

**My solution**: An agentic system that **generates and executes Python code** dynamically.

### How It Works

The coding agent uses a **ReAct (Reasoning + Acting) loop**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User Query: "Compare sales by location"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. LLM Reasoning:                                       â”‚
â”‚    - Need to query orders table                        â”‚
â”‚    - Group by location                                  â”‚
â”‚    - Calculate total sales                             â”‚
â”‚    - Compare results                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Code Generation:                                     â”‚
â”‚    ```python                                            â”‚
â”‚    import pandas as pd                                  â”‚
â”‚    from db_helper import get_db_connection              â”‚
â”‚                                                         â”‚
â”‚    conn = get_db_connection()                           â”‚
â”‚    df = pd.read_sql("""                                 â”‚
â”‚        SELECT location_name,                            â”‚
â”‚               SUM(total_amount) as sales                â”‚
â”‚        FROM orders                                      â”‚
â”‚        GROUP BY location_name                           â”‚
â”‚    """, conn)                                           â”‚
â”‚    print(df.to_json())                                  â”‚
â”‚    ```                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Code Execution (Isolated Sandbox)                    â”‚
â”‚    - Runs in Docker container                           â”‚
â”‚    - Limited permissions                                â”‚
â”‚    - Database read-only access                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Result Processing:                                   â”‚
â”‚    - Parse JSON output                                  â”‚
â”‚    - Determine chart type (bar, line, pie, etc.)        â”‚
â”‚    - Generate visualization                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Response to User:                                    â”‚
â”‚    - Interactive chart widget                           â”‚
â”‚    - Text summary                                       â”‚
â”‚    - Data table (if applicable)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agentic Loop Details

The agent can iterate multiple times:

1. **Initial Query** â†’ Generate code
2. **Execute Code** â†’ Get results
3. **Analyze Results** â†’ If incomplete or error:
   - **Refine code** based on error messages
   - **Adjust query** based on data structure
   - **Retry execution**
4. **Final Output** â†’ Return visualization

**Why this works better:**
- **Adaptive**: Adjusts approach based on actual data
- **Self-correcting**: Fixes errors automatically
- **Flexible**: Can handle unexpected data structures
- **Powerful**: Full Python ecosystem available (pandas, matplotlib, etc.)

---

## ğŸ§  Context Compression: Managing Long Conversations

### The Problem

As users ask multiple questions, the conversation history grows:
- Each message adds tokens to the LLM context
- OpenAI has token limits (e.g., 60,000 tokens)
- Long histories become expensive and slow
- **But**: We need to remember past context for follow-up questions

### The Solution: Intelligent Compression

Instead of truncating history, we **compress it intelligently**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Before Compression:                                      â”‚
â”‚ - 50 messages                                            â”‚
â”‚ - 45,000 tokens                                          â”‚
â”‚ - Full conversation history                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼ (When >70% of token limit)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compression Process:                                    â”‚
â”‚ 1. Identify compressible messages (early in history)   â”‚
â”‚ 2. Send to LLM with compression prompt                 â”‚
â”‚ 3. Extract structured snapshot:                        â”‚
â”‚    - Overall goal                                        â”‚
â”‚    - Key knowledge                                       â”‚
â”‚    - File system state                                   â”‚
â”‚    - Recent actions                                      â”‚
â”‚    - Current plan                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ After Compression:                                      â”‚
â”‚ - 2 messages (snapshot + acknowledgment)                â”‚
â”‚ - ~2,000 tokens                                          â”‚
â”‚ - All essential context preserved                        â”‚
â”‚ - Recent messages kept in full                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Approach?

**Traditional truncation** (keep last N messages):
- âŒ Loses important early context
- âŒ User might reference something from earlier
- âŒ Agent "forgets" its own decisions

**My compression approach**:
- âœ… Preserves essential information
- âœ… Maintains user goals and constraints
- âœ… Keeps recent context in full detail
- âœ… Reduces token usage by ~95%

### Compression Threshold

We compress when usage exceeds **70% of token limit** (42,000 / 60,000 tokens):
- **Why 70%?** Leaves room for new queries and responses
- **What gets compressed?** Early messages (oldest first)
- **What stays?** Recent messages and tool results

---

## ğŸ“Š Data Architecture

### Source Data

The system processes **6 JSON files** from 3 different POS systems:

| Source | Files | Challenge |
|--------|-------|-----------|
| **Toast POS** | `toast_pos_export.json` | Single nested structure |
| **DoorDash** | `doordash_orders.json` | Delivery-specific fields |
| **Square POS** | 4 files (catalog, orders, payments, locations) | Split across multiple files |

**Data Quality Issues:**
- Inconsistent product names ("Hash Browns" vs "Hashbrowns")
- Typos ("Griled Chiken", "expresso")
- Category variations ("ğŸ” Burgers" vs "Burgers")
- Different date/time formats

### Normalized Schema

We designed a unified schema that handles all sources:

```
locations (id, name, address, ...)
    â”‚
    â”œâ”€â”€ orders (id, location_id, order_date, order_type, ...)
    â”‚       â”‚
    â”‚       â”œâ”€â”€ order_items (id, order_id, product_id, quantity, price, ...)
    â”‚       â”‚
    â”‚       â””â”€â”€ payments (id, order_id, payment_type, amount, ...)
    â”‚
    â””â”€â”€ products (id, location_id, name, category, normalized_name, ...)
```

**Why this design?**
- **Normalized**: Reduces data duplication
- **Flexible**: Handles different order types (dine-in, delivery, pickup)
- **Queryable**: Optimized for analytics queries
- **Extensible**: Easy to add new data sources

### ETL Process

1. **Extract**: Parse JSON files with source-specific logic
2. **Transform**: 
   - Normalize product names (fuzzy matching)
   - Standardize dates/times
   - Convert currencies
   - Handle missing fields
3. **Load**: Insert into Supabase with referential integrity

**ETL Functions** (PostgreSQL):
- `normalize_product_name()`: Fuzzy matching for product names
- `calculate_order_totals()`: Aggregations
- `detect_duplicate_orders()`: Data quality checks

---

## ğŸš€ Easy Setup (Local Development)

### Prerequisites

- Docker & Docker Compose
- Git

### Quick Start

```bash
# 1. Clone the repository
git clone <your-repo-url>
cd clave-take-home

# 2. Create .env file (copy from .env.example)
cp .env.example .env
# Edit .env with your:
# - Supabase credentials (if using production DB)
# - OpenAI API key

# 3. Start all services
docker-compose up -d

# 4. Run database migrations
cd my-api
./run_alembic_migrations.sh

# 5. Install ETL functions
./install_etl_functions.sh

# 6. Load data
./load_data.sh

# 7. Access the application
# Frontend: http://localhost:3000
# API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

### Services

| Service | Port | Description |
|---------|------|-------------|
| Frontend | 3000 | Next.js dashboard |
| Backend API | 8000 | FastAPI server |
| Code Executor | 8001 | Isolated Python executor |
| Redis | 6379 | Session storage |
| PostgreSQL | 5432 | Local database (if not using Supabase) |

### Database Setup

**Option A: Local PostgreSQL (Docker)**
- Already included in `docker-compose.yml`
- No additional setup needed

**Option B: Supabase (Production)**
1. Create project at [supabase.com](https://supabase.com)
2. Get connection string from Settings â†’ Database
3. Update `.env` with Supabase credentials
4. Use `docker-compose.prod.yml` instead

---

## ğŸ“ Key Scripts

### Database Migrations

```bash
./my-api/run_alembic_migrations.sh
```

**What it does:**
- Runs Alembic migrations to create/update schema
- Handles both local PostgreSQL and Supabase
- Automatically detects database type from `.env`

**Why Alembic?**
- Version control for schema changes
- Reproducible deployments
- Rollback capability

### ETL Functions Installation

```bash
./my-api/install_etl_functions.sh
```

**What it does:**
- Installs PostgreSQL functions for data normalization
- Functions: `normalize_product_name()`, `calculate_order_totals()`, etc.

**Why PostgreSQL functions?**
- **Performance**: Runs close to data (no network overhead)
- **Consistency**: Same logic for ETL and queries
- **Reusability**: Can be called from Python or SQL

### Data Loading

```bash
./my-api/load_data.sh
```

**What it does:**
1. Parses all 6 JSON files
2. Cleans and normalizes data
3. Inserts into database
4. Validates data integrity

**ETL Process:**
- **Toast**: Single file â†’ orders, items, payments
- **DoorDash**: Orders â†’ normalized format
- **Square**: 4 files â†’ unified schema

---

## ğŸ› ï¸ Technology Stack

### Frontend
- **Next.js 14** (App Router) - React framework
- **TypeScript** - Type safety
- **Recharts** - Chart library
- **Tailwind CSS** - Styling

### Backend
- **FastAPI** - Python web framework
- **OpenAI API** - LLM for code generation
- **SQLAlchemy** - ORM for database
- **Alembic** - Database migrations

### Infrastructure
- **Docker** - Containerization
- **Supabase** - PostgreSQL database (production)
- **Redis** - Session management
- **Docker Compose** - Local development

### Why These Choices?

**FastAPI over Flask/Django:**
- Async support for concurrent requests
- Automatic API documentation
- Type hints for better IDE support

**Next.js App Router:**
- Server components for better performance
- Built-in API routes (though we use separate backend)
- Excellent TypeScript support

**Supabase:**
- Real PostgreSQL (not a toy database)
- Easy to evaluate schema design
- Production-ready from day one

---

## ğŸ¨ Design Decisions

### 1. Separate Code Executor Service

**Why?** Security and isolation.

- Code execution is **dangerous** (arbitrary Python code)
- Isolated Docker container prevents:
  - File system access
  - Network access (except database)
  - Resource exhaustion
- **Alternative considered**: In-process execution â†’ Rejected (too risky)

### 2. Agentic Loop vs Direct SQL

**Why agentic?** Flexibility and adaptability.

- Can handle ambiguous queries
- Self-corrects errors
- Can use full Python ecosystem (pandas, matplotlib)
- **Alternative considered**: Query templates â†’ Too rigid

### 3. Context Compression

**Why compress instead of truncate?** Preserves important context.

- User goals and constraints remembered
- Agent's own decisions preserved
- Recent context kept in full
- **Alternative considered**: Simple truncation â†’ Loses too much

### 4. Normalized Database Schema

**Why normalized?** Data integrity and query flexibility.

- Single source of truth for products/locations
- Easy to add new data sources
- Efficient for analytics queries
- **Alternative considered**: Denormalized â†’ Too much duplication

---

## ğŸ“ˆ What Makes This Production-Ready

1. **Error Handling**: Comprehensive try/catch with user-friendly messages
2. **Security**: Isolated code execution, input validation
3. **Scalability**: Stateless API, Redis for sessions
4. **Monitoring**: Structured logging, health checks
5. **Database**: Proper migrations, indexes, constraints
6. **Documentation**: Code comments, API docs, README

---

## ğŸ”® Future Improvements

Given more time, we would:

1. **Caching**: Cache query results for common questions
2. **Query Optimization**: Analyze and optimize generated SQL
3. **Multi-user Support**: User authentication and data isolation
4. **Advanced Visualizations**: More chart types, custom styling
5. **Query History**: Save and replay previous queries
6. **Data Refresh**: Automated ETL pipeline for new data

---

**Built with â¤ï¸ for Clave Engineering Assessment**
