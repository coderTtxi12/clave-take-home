# Natural Language Dashboard Generator

A production-ready AI-powered dashboard that transforms natural language queries into dynamic visualizations from restaurant analytics data. Built for the Clave Engineering Take-Home Assessment.

---

## ğŸš€ Live Application

**The application is live and ready for testing:**

### ğŸŒ [**http://44.222.146.210:3000/**](http://44.222.146.210:3000/)

You can test the full functionality including:
- Natural language queries
- Dynamic chart generation
- Data analysis from restaurant analytics
- Interactive dashboard features

---

## ğŸ¯ Overview

This system allows restaurant owners to ask questions in plain English like:
- *"Show me sales comparison between Downtown and Airport locations"*
- *"What were my top 5 selling products last week?"*
- *"Graph hourly sales for Friday vs Saturday at all stores"*

**The system automatically:**
1. Parses the natural language query
2. Generates Python code to query the database
3. Executes the code in a secure environment
4. Creates dynamic visualizations using Python libraries - the agent can generate **any type of visualization** (bar charts, line graphs, pie charts, heatmaps, scatter plots, tables, metrics, etc.) because it writes and executes custom code
5. Returns the visualization to the dashboard

---

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend       â”‚  Next.js Dashboard (TypeScript)
â”‚   (Port 3000)    â”‚  - Natural language input
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Dynamic visualization rendering
         â”‚
         â”‚ HTTP/REST
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend API   â”‚  FastAPI (Python)
â”‚   (Port 8000)   â”‚  - Query processing
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Session management
         â”‚
         â”‚ OpenAI API
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Coding Agent   â”‚  Agentic Loop
â”‚                 â”‚  - Code generation
â”‚                 â”‚  - Tool execution
â”‚                 â”‚  - Context compression
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Code Execution
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Code Executor   â”‚  Isolated Docker Container
â”‚ (Port 8001)     â”‚  - Secure code execution
â”‚                 â”‚  - Database access
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ SQL Queries
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Supabase      â”‚  PostgreSQL Database
â”‚   (Production)  â”‚  - Normalized restaurant data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **User Query** â†’ Frontend sends natural language query
2. **LLM Processing** â†’ Backend uses OpenAI to interpret query
3. **Code Generation** â†’ Agent generates Python code with database queries
4. **Code Execution** â†’ Isolated executor runs code safely
5. **Data Retrieval** â†’ Executor queries Supabase database
6. **Visualization** â†’ Results formatted as charts/tables
7. **Response** â†’ Frontend renders interactive widgets

---

## ğŸ¤– The Coding Agent: Core Innovation

### Why a Coding Agent?

Traditional approaches map queries directly to SQL, but this is brittle:
- **Limited flexibility**: Can't handle complex data transformations
- **No iteration**: Can't refine queries based on results
- **Fixed patterns**: Struggles with ambiguous or novel requests

**My solution**: An agentic system that **generates and executes Python code** dynamically.

### How It Works

The coding agent uses a **ReAct (Reasoning + Acting) loop**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User Query: "Compare sales by location"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. LLM Reasoning:                                       â”‚
â”‚    - Need to query orders table                        â”‚
â”‚    - Group by location                                  â”‚
â”‚    - Calculate total sales                             â”‚
â”‚    - Compare results                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Code Generation:                                     â”‚
â”‚    ```python                                            â”‚
â”‚    import pandas as pd                                  â”‚
â”‚    from db_helper import get_db_connection              â”‚
â”‚                                                         â”‚
â”‚    conn = get_db_connection()                           â”‚
â”‚    df = pd.read_sql("""                                 â”‚
â”‚        SELECT location_name,                            â”‚
â”‚               SUM(total_amount) as sales                â”‚
â”‚        FROM orders                                      â”‚
â”‚        GROUP BY location_name                           â”‚
â”‚    """, conn)                                           â”‚
â”‚    print(df.to_json())                                  â”‚
â”‚    ```                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Code Execution (Isolated Sandbox)                    â”‚
â”‚    - Runs in Docker container                           â”‚
â”‚    - Limited permissions                                â”‚
â”‚    - Database read-only access                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Result Processing:                                   â”‚
â”‚    - Parse JSON output                                  â”‚
â”‚    - Determine chart type (bar, line, pie, etc.)        â”‚
â”‚    - Generate visualization                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Response to User:                                    â”‚
â”‚    - Interactive chart widget                           â”‚
â”‚    - Text summary                                       â”‚
â”‚    - Data table (if applicable)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agentic Loop Details

The agent can iterate multiple times:

1. **Initial Query** â†’ Generate code
2. **Execute Code** â†’ Get results
3. **Analyze Results** â†’ If incomplete or error:
   - **Refine code** based on error messages
   - **Adjust query** based on data structure
   - **Retry execution**
4. **Final Output** â†’ Return visualization

**Why this works better:**
- **Adaptive**: Adjusts approach based on actual data
- **Self-correcting**: Fixes errors automatically
- **Flexible**: Can handle unexpected data structures
- **Powerful**: Full Python ecosystem available (pandas, matplotlib, etc.)

---

## ğŸ§  Context Compression: Managing Long Conversations

### The Problem

As users ask multiple questions, the conversation history grows:
- Each message adds tokens to the LLM context
- OpenAI has token limits (e.g., 60,000 tokens)
- Long histories become expensive and slow
- **But**: We need to remember past context for follow-up questions

### The Solution: Intelligent Compression

Instead of truncating history, we **compress it intelligently**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Before Compression:                                      â”‚
â”‚ - 50 messages                                            â”‚
â”‚ - 45,000 tokens                                          â”‚
â”‚ - Full conversation history                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼ (When >70% of token limit)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compression Process:                                    â”‚
â”‚ 1. Identify compressible messages (early in history)   â”‚
â”‚ 2. Send to LLM with compression prompt                 â”‚
â”‚ 3. Extract structured snapshot:                        â”‚
â”‚    - Overall goal                                        â”‚
â”‚    - Key knowledge                                       â”‚
â”‚    - File system state                                   â”‚
â”‚    - Recent actions                                      â”‚
â”‚    - Current plan                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ After Compression:                                      â”‚
â”‚ - 2 messages (snapshot + acknowledgment)                â”‚
â”‚ - ~2,000 tokens                                          â”‚
â”‚ - All essential context preserved                        â”‚
â”‚ - Recent messages kept in full                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Approach?

**Traditional truncation** (keep last N messages):
- âŒ Loses important early context
- âŒ User might reference something from earlier
- âŒ Agent "forgets" its own decisions

**My compression approach**:
- âœ… Preserves essential information
- âœ… Maintains user goals and constraints
- âœ… Keeps recent context in full detail
- âœ… Reduces token usage by ~95%

### Compression Threshold

We compress when usage exceeds **70% of token limit** (42,000 / 60,000 tokens):
- **Why 70%?** Leaves room for new queries and responses
- **What gets compressed?** Early messages (oldest first)
- **What stays?** Recent messages and tool results

---

## ğŸ“Š Data Architecture

### Source Data

The system processes **6 JSON files** from 3 different POS systems:

| Source | Files | Challenge |
|--------|-------|-----------|
| **Toast POS** | `toast_pos_export.json` | Single nested structure |
| **DoorDash** | `doordash_orders.json` | Delivery-specific fields |
| **Square POS** | 4 files (catalog, orders, payments, locations) | Split across multiple files |

**Data Quality Issues:**
- Inconsistent product names ("Hash Browns" vs "Hashbrowns")
- Typos ("Griled Chiken", "expresso")
- Category variations ("ğŸ” Burgers" vs "Burgers")
- Different date/time formats

### Normalized Schema

Designed a unified schema that handles all sources:

```
locations (id, name, address, ...)
    â”‚
    â”œâ”€â”€ orders (id, location_id, order_date, order_type, ...)
    â”‚       â”‚
    â”‚       â”œâ”€â”€ order_items (id, order_id, product_id, quantity, price, ...)
    â”‚       â”‚
    â”‚       â””â”€â”€ payments (id, order_id, payment_type, amount, ...)
    â”‚
    â””â”€â”€ products (id, location_id, name, category, normalized_name, ...)
```

**Why this design?**
- **Normalized**: Reduces data duplication
- **Flexible**: Handles different order types (dine-in, delivery, pickup)
- **Queryable**: Optimized for analytics queries
- **Extensible**: Easy to add new data sources

### ETL Process

1. **Extract**: Parse JSON files with source-specific logic
2. **Transform**: 
   - Normalize product names (fuzzy matching)
   - Standardize dates/times
   - Convert currencies
   - Handle missing fields
3. **Load**: Insert into Supabase with referential integrity

**ETL Functions** (PostgreSQL):
- `normalize_product_name()`: Fuzzy matching for product names
- `calculate_order_totals()`: Aggregations
- `detect_duplicate_orders()`: Data quality checks

---

## ğŸš€ Easy Setup (Local Development)

### Prerequisites

- Docker & Docker Compose
- Git

### Quick Start (Less than 5 Minutes)

```bash
# 1. Clone the repository
git clone https://github.com/coderTtxi12/clave-take-home.git
cd clave-take-home

# 2. Create .env file (copy from .env.example)
cp .env.example .env
# Edit .env with your:
# - OpenAI API key

# 3. Start all services
docker compose up -d

# 4. Run Alembic migrations âœ…
docker exec -it restaurant_analytics_api alembic upgrade head

# 5. Create read-only user (if it doesn't exist) âœ…
docker exec -it restaurant_analytics_db psql -U postgres -d restaurant_analytics -f /sql/create_readonly_user.sql

# 6. Install ETL functions âœ…
docker exec -it restaurant_analytics_db psql -U postgres -d restaurant_analytics -f /sql/etl_functions.sql

# 7. Load data âœ…
docker exec -it restaurant_analytics_api bash -c "cd /app/scripts && python3 load_all_data.py --clear"

# 8. Access the application
# Frontend: http://localhost:3000
# API: http://localhost:8000
```

### Services

| Service | Port | Description |
|---------|------|-------------|
| Frontend | 3000 | Next.js dashboard |
| Backend API | 8000 | FastAPI server |
| Code Executor | 8001 | Isolated Python executor |
| Redis | 6379 | Session storage |
| PostgreSQL | 5432 | Local database (if not using Supabase) |

### Database Setup

**Option A: Local PostgreSQL (Docker)**
- Already included in `docker-compose.yml`
- No additional setup needed

**Option B: Supabase (Production)**
1. Create project at [supabase.com](https://supabase.com)
2. Get connection string from Settings â†’ Database
3. Update `.env` with Supabase credentials
4. Use `docker-compose.prod.yml` instead

---

## ğŸ“ Project Structure

```
clave-take-home/
â”œâ”€â”€ data/                          # Source data files (JSON from POS systems)
â”‚   â””â”€â”€ sources/
â”‚       â”œâ”€â”€ toast_pos_export.json  # Toast POS data
â”‚       â”œâ”€â”€ doordash_orders.json   # DoorDash orders
â”‚       â””â”€â”€ square/                # Square POS data (4 files)
â”‚           â”œâ”€â”€ catalog.json
â”‚           â”œâ”€â”€ locations.json
â”‚           â”œâ”€â”€ orders.json
â”‚           â””â”€â”€ payments.json
â”‚
â”œâ”€â”€ my-api/                        # Backend API (FastAPI)
â”‚   â”œâ”€â”€ app/                       # Main application code
â”‚   â”‚   â”œâ”€â”€ api/routes/           # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ coding_agent.py   # Main coding agent endpoint
â”‚   â”‚   â”‚   â””â”€â”€ health.py          # Health check endpoint
â”‚   â”‚   â”œâ”€â”€ core/                  # Core configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py         # Settings and environment variables
â”‚   â”‚   â”‚   â””â”€â”€ logging.py        # Logging configuration
â”‚   â”‚   â”œâ”€â”€ models/                # Pydantic models and database models
â”‚   â”‚   â”‚   â”œâ”€â”€ coding_agent.py   # Request/response models
â”‚   â”‚   â”‚   â””â”€â”€ database.py       # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ services/              # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ coding_agent_service.py  # Agentic loop implementation
â”‚   â”‚   â”‚   â””â”€â”€ session_manager.py        # Redis session management
â”‚   â”‚   â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”‚   â”‚   â”œâ”€â”€ code_executor.py  # Code execution client
â”‚   â”‚   â”‚   â”œâ”€â”€ image_processor.py # Image extraction/processing
â”‚   â”‚   â”‚   â””â”€â”€ tools.py           # Agent tools (execute_code, etc.)
â”‚   â”‚   â””â”€â”€ main.py                # FastAPI application entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ code-executor/             # Isolated code execution service
â”‚   â”‚   â”œâ”€â”€ executor.py            # FastAPI service for code execution
â”‚   â”‚   â”œâ”€â”€ db_helper.py          # Read-only database connection
â”‚   â”‚   â””â”€â”€ Dockerfile             # Container definition
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/                    # ETL scripts
â”‚   â”‚   â”œâ”€â”€ load_all_data.py       # Master ETL orchestrator
â”‚   â”‚   â”œâ”€â”€ load_toast_data.py     # Toast POS ETL
â”‚   â”‚   â”œâ”€â”€ load_doordash_data.py  # DoorDash ETL
â”‚   â”‚   â”œâ”€â”€ load_square_data.py    # Square POS ETL
â”‚   â”‚   â””â”€â”€ etl_utils.py           # Shared ETL utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ alembic/                   # Database migrations
â”‚   â”‚   â”œâ”€â”€ env.py                 # Alembic environment config
â”‚   â”‚   â””â”€â”€ versions/              # Migration files
â”‚   â”‚
â”‚   â”œâ”€â”€ sql/                        # SQL scripts
â”‚   â”‚   â”œâ”€â”€ etl_functions.sql      # PostgreSQL functions for ETL
â”‚   â”‚   â””â”€â”€ create_readonly_user.sql  # Read-only user creation
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                    # LLM prompts
â”‚   â”‚   â”œâ”€â”€ prompts.py             # System prompts for agent
â”‚   â”‚   â””â”€â”€ DB_SCHEMA.md           # Database schema documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ run_alembic_migrations.sh  # Script: Run database migrations
â”‚   â”œâ”€â”€ install_etl_functions.sh   # Script: Install PostgreSQL functions
â”‚   â”œâ”€â”€ load_data.sh               # Script: Load ETL data
â”‚   â”œâ”€â”€ setup_production.sh        # Script: Complete production setup
â”‚   â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚   â””â”€â”€ Dockerfile                 # Backend API container
â”‚
â”œâ”€â”€ my-dashboard/                   # Frontend (Next.js)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/                   # Next.js App Router
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx          # Main page component
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx        # Root layout
â”‚   â”‚   â”‚   â””â”€â”€ api/              # Next.js API routes (proxy)
â”‚   â”‚   â”‚       â””â”€â”€ coding-agent/query/route.ts
â”‚   â”‚   â”œâ”€â”€ components/            # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ Header/           # App header with theme toggle
â”‚   â”‚   â”‚   â”œâ”€â”€ WelcomeSection/   # Initial welcome screen
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageList/      # Active conversation view
â”‚   â”‚   â”‚   â”œâ”€â”€ Message/          # Individual message component
â”‚   â”‚   â”‚   â”œâ”€â”€ InputForm/        # Chat input form
â”‚   â”‚   â”‚   â”œâ”€â”€ FilePreview/      # File attachment preview
â”‚   â”‚   â”‚   â””â”€â”€ TypingIndicator/   # Loading indicator
â”‚   â”‚   â”œâ”€â”€ hooks/                 # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useChat.ts        # Chat state management
â”‚   â”‚   â”‚   â””â”€â”€ useTheme.ts       # Theme management
â”‚   â”‚   â””â”€â”€ types/                 # TypeScript type definitions
â”‚   â”œâ”€â”€ package.json               # Node.js dependencies
â”‚   â”œâ”€â”€ next.config.ts             # Next.js configuration
â”‚   â””â”€â”€ Dockerfile                 # Frontend container
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ EXAMPLE_QUERIES.md         # Example natural language queries
â”‚   â””â”€â”€ SCHEMA_HINTS.md            # Database schema hints
â”‚
â”œâ”€â”€ docker-compose.yml              # Docker Compose for local development
â”œâ”€â”€ DB_SCHEMA.md                   # Database schema documentation
â””â”€â”€ README.md                      # This file
```

---

## ğŸ“ Key Scripts

### Database Migration Script

**File**: `my-api/run_alembic_migrations.sh`

```bash
./my-api/run_alembic_migrations.sh
```

**What it does:**
- Runs Alembic migrations to create/update database schema
- Automatically detects database type (local PostgreSQL or Supabase) from `.env`
- Handles environment variable loading and validation
- Installs Python dependencies if missing (alembic, sqlalchemy, psycopg2)

**Usage:**
- `./run_alembic_migrations.sh upgrade head` - Apply all migrations
- `./run_alembic_migrations.sh current` - Show current version
- `./run_alembic_migrations.sh history` - Show migration history

**Why Alembic?**
- Version control for schema changes
- Reproducible deployments
- Rollback capability
- Team collaboration on schema changes

### ETL Functions Installation Script

**File**: `my-api/install_etl_functions.sh`

```bash
./my-api/install_etl_functions.sh
```

**What it does:**
- Installs PostgreSQL functions for data normalization and ETL operations
- Functions include:
  - `get_or_create_category()`: Normalize and deduplicate categories
  - `get_location_id_by_source()`: Map source-specific location IDs
  - `validate_etl_data()`: Data quality checks
- Works with both local PostgreSQL and Supabase
- Uses Docker `psql` if local `psql` is not available

**Why PostgreSQL functions?**
- **Performance**: Runs close to data (no network overhead)
- **Consistency**: Same logic for ETL and queries
- **Reusability**: Can be called from Python or SQL
- **Data integrity**: Enforced at database level

### Data Loading Script

**File**: `my-api/load_data.sh`

```bash
./my-api/load_data.sh [--clear]
```

**What it does:**
1. Validates all source JSON files exist
2. Checks database schema is initialized
3. Verifies ETL functions are installed
4. Runs master ETL pipeline (`load_all_data.py`)
5. Loads data from all sources in correct order:
   - Toast POS â†’ DoorDash â†’ Square POS

**Options:**
- `--clear` or `-c`: Deletes all existing data before loading (fresh start)
- Without flag: Appends data (may create duplicates if re-run)

**ETL Process:**
- **Toast**: Single nested JSON â†’ normalized orders, items, payments
- **DoorDash**: Delivery orders â†’ unified order format
- **Square**: 4 separate files â†’ unified schema with referential integrity

### Production Setup Script

**File**: `my-api/setup_production.sh`

```bash
./my-api/setup_production.sh
```

**What it does:**
- Orchestrates complete production setup in one command:
  1. Runs Alembic migrations
  2. Creates read-only database user
  3. Installs ETL functions
  4. Prompts for data loading (with `--clear` option)

**Features:**
- Robust error handling with clear messages
- Automatic dependency installation
- Works with both local PostgreSQL and Supabase
- Interactive prompts for data loading confirmation

**Use case**: First-time setup or fresh deployment

### Other Scripts

**`my-api/run_migrations.sh`**: Alternative migration runner (legacy)

**`my-api/TEST_COMMANDS.sh`**: Test commands for development/debugging

**`my-api/run.py`**: Alternative application entry point (development)

---

## ğŸ› ï¸ Technology Stack

### Frontend
- **Next.js 14** (App Router) - React framework
- **TypeScript** - Type safety
- **Recharts** - Chart library
- **Tailwind CSS** - Styling

### Backend
- **FastAPI** - Python web framework
- **OpenAI API** - LLM for code generation
- **SQLAlchemy** - ORM for database
- **Alembic** - Database migrations

### Infrastructure
- **Docker** - Containerization
- **Supabase** - PostgreSQL database (production)
- **Redis** - Session management
- **Docker Compose** - Local development

### Why These Choices?

**FastAPI over Flask/Django:**
- Async support for concurrent requests
- Automatic API documentation
- Type hints for better IDE support

**Next.js App Router:**
- Server components for better performance
- Built-in API routes (though we use separate backend)
- Excellent TypeScript support

**Supabase:**
- Real PostgreSQL (not a toy database)
- Easy to evaluate schema design
- Production-ready from day one

---

## ğŸ¨ Design Decisions

### 1. Separate Code Executor Service

**Why?** Security and isolation.

- Code execution is **dangerous** (arbitrary Python code)
- Isolated Docker container prevents:
  - File system access
  - Network access (except database)
  - Resource exhaustion
- **Alternative considered**: In-process execution â†’ Rejected (too risky)

### 2. Agentic Loop vs Direct SQL

**Why agentic?** Flexibility and adaptability.

- Can handle ambiguous queries
- Self-corrects errors
- Can use full Python ecosystem (pandas, matplotlib)
- **Alternative considered**: Query templates â†’ Too rigid

### 3. Context Compression

**Why compress instead of truncate?** Preserves important context.

- User goals and constraints remembered
- Agent's own decisions preserved
- Recent context kept in full
- **Alternative considered**: Simple truncation â†’ Loses too much

### 4. Normalized Database Schema

**Why normalized?** Data integrity and query flexibility.

- Single source of truth for products/locations
- Easy to add new data sources
- Efficient for analytics queries
- **Alternative considered**: Denormalized â†’ Too much duplication

---

## ğŸ“ˆ What Makes This Production-Ready

1. **Error Handling**: Comprehensive try/catch with user-friendly messages
2. **Security**: Isolated code execution, input validation
3. **Scalability**: Stateless API, Redis for sessions
4. **Monitoring**: Structured logging, health checks
5. **Database**: Proper migrations, indexes, constraints
6. **Documentation**: Code comments, API docs, README

---

## ğŸ”® Future Improvements

Given more time:

1. **Caching**: Cache query results for common questions
2. **Query Optimization**: Analyze and optimize generated SQL
3. **Query History**: Save and replay previous queries
4. **Data Refresh**: Automated ETL pipeline for new data
5. **Fullstack Agent**: Complete the fullstack agent implementation - a powerful tool that enables users to create any type of data analytics report through natural language, similar to how Lovable works for app development but specialized for data analytics workflows

---

## ğŸ“š More Documentation

For comprehensive details about the database architecture, normalization process, and schema design, see:

### [**Database Schema Reference**](./DB_SCHEMA.md)

This document contains:

- **ğŸ“Š Database Schema Overview**: Complete explanation of how data from three different POS systems (Toast, DoorDash, Square) is unified into a single normalized schema
- **ğŸ”„ Normalization Process**: Detailed breakdown of the multi-layer normalization approach:
  - Python ETL scripts (`etl_utils.py`) with normalization functions
  - PostgreSQL functions (`etl_functions.sql`) for category and location mapping
  - Fuzzy matching using `pg_trgm` extension
- **ğŸ“ˆ Visual Database Diagram**: ASCII diagram showing all relationships (1:N, 1:1, N:1, self-references)
- **ğŸ“‹ Complete Table Reference**: Detailed documentation of all tables, columns, relationships, and indexes
- **ğŸ” Query Guidance**: Best practices for common query patterns (sales, dates, locations, products, payments)
- **ğŸ“ Important Notes**: CASCADE deletes, triggers, JSONB usage, denormalization strategy

**Key Topics Covered:**
- How product names are normalized across sources ("Hashbrowns" â†’ "Hash Browns")
- How categories are unified ("ğŸ” Burgers" â†’ "Burgers", "Drinks" â†’ "Beverages")
- Location mapping strategy using JSONB `source_ids`
- Product mapping table for maintaining source-specific IDs
- DoorDash merchant payout handling
- Date/time field usage patterns

---

**Built with â¤ï¸ for Clave Engineering Assessment**
